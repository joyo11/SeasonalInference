{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyo11/SeasonalInference/blob/main/DLCV_final_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsuZABz_hIgv"
      },
      "outputs": [],
      "source": [
        "!pip install torchgeo crcmod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B9b2g4khchN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import random\n",
        "import torch\n",
        "import timm\n",
        "import torchgeo\n",
        "import rasterio\n",
        "# import kornia\n",
        "import pickle\n",
        "# import tempfile\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchgeo.datasets import RasterDataset#, stack_samples, unbind_samples, Landsat8\n",
        "from torchgeo import models\n",
        "from torchgeo.trainers import ClassificationTask\n",
        "from torchgeo.models import ResNet18_Weights\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# from torchvision import transforms\n",
        "from datetime import datetime\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the service account key file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "i1LRfbPzHT4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/dl-for-cv-final-project-f69fa12b7fae.json'"
      ],
      "metadata": {
        "id": "4p6fvzryIpLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth activate-service-account --key-file='/content/dl-for-cv-final-project-f69fa12b7fae.json'"
      ],
      "metadata": {
        "id": "xBckEpfePC2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKY_vwj4xc4F"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UfCJWNGzoT-"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'dlcv_finalproj_data1'\n",
        "folder_path = 'imgcollect'\n",
        "\n",
        "!gsutil -m -o 'GSUtil:sliced_object_download_threshold=150M' cp -r gs://{bucket_name}/{folder_path} /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'dlcv_finalproj_data1'\n",
        "folder_path = 'random_samples/LC08_014029_20191211.pkl'\n",
        "!gsutil -m -o 'GSUtil:sliced_object_download_threshold=150M' cp -r gs://{bucket_name}/{folder_path} /content"
      ],
      "metadata": {
        "id": "N-nmTI8pRpLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'dlcv_finalproj_data1'\n",
        "file_path = 'combined_data.pkl'\n",
        "!gsutil -m -o 'GSUtil:sliced_object_download_threshold=150M' cp -r gs://{bucket_name}/{file_path} /content"
      ],
      "metadata": {
        "id": "Ss6swWA1X_i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/combined_data.pkl\", 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "cGG0bo8VaPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = data[4678]\n",
        "plot_image(image)\n",
        "if label==1:\n",
        "    month='January'\n",
        "elif label ==2:\n",
        "    month=\"February\"\n",
        "elif label ==3:\n",
        "    month=\"March\"\n",
        "elif label ==4:\n",
        "    month=\"April\"\n",
        "elif label ==5:\n",
        "    month=\"May\"\n",
        "elif label ==6:\n",
        "    month=\"June\"\n",
        "elif label ==7:\n",
        "    month=\"July\"\n",
        "elif label ==8:\n",
        "    month=\"August\"\n",
        "elif label ==9:\n",
        "    month=\"September\"\n",
        "elif label ==10:\n",
        "    month=\"October\"\n",
        "elif label ==11:\n",
        "    month=\"November\"\n",
        "else:\n",
        "    month=\"December\"\n",
        "print(\"Label =\",label,\"Month =\",month)\n"
      ],
      "metadata": {
        "id": "W5jEZBRduRMY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d5f780b9-ee2d-4d6b-f10d-a54b3be45949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-95f01b1d1c40>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4678\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[:10000]\n",
        "val_data = data[10001:11250]\n",
        "test_data = data[11250:]"
      ],
      "metadata": {
        "id": "CQ5b3qYGbNWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=64)\n",
        "val_loader = DataLoader(val_data, batch_size=64)\n",
        "test_loader = DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "z9e5QfmEbtyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qgDaMx6M1zs"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loBbDaoTDuj5"
      },
      "outputs": [],
      "source": [
        "class LandsatCustom(RasterDataset):\n",
        "    is_image = True\n",
        "    separate_files = False\n",
        "    all_bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B9\",\"B10\",\"B11\"]\n",
        "    rgb_bands = [\"B4\", \"B3\", \"B2\"]\n",
        "    filename_regex = r'(?P<satellite>[A-Za-z0-9]+)_(?P<number>\\d+)_(?P<date>(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2}))\\.tif'\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        super(LandsatCustom, self).__init__(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        xmin = self.index.bounds[0] + (self.index.bounds[1] - self.index.bounds[0])/5\n",
        "        xmax = self.index.bounds[1] - (self.index.bounds[1] - self.index.bounds[0])/5\n",
        "        ymin = self.index.bounds[2] + (self.index.bounds[3] - self.index.bounds[2])/5\n",
        "        ymax = self.index.bounds[3] - (self.index.bounds[3] - self.index.bounds[2])/5\n",
        "        newroi = torchgeo.datasets.BoundingBox(xmin, xmax, ymin, ymax, self.index.bounds[4], self.index.bounds[5])\n",
        "        # sampler = RandomGeoSampler(self, roi=newroi, size=256, length=5)\n",
        "        # self.dataset = []\n",
        "        # for index in sampler:\n",
        "        #   sample = self[index]\n",
        "        #   self.dataset.append(sample)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raster_data = super(LandsatCustom, self).__getitem__(index)\n",
        "        image = raster_data['image'][:7,:,:]\n",
        "        label = datetime.utcfromtimestamp(self.index.bounds[4]).month\n",
        "        return (image, label)\n",
        "\n",
        "    def get_samples(self):\n",
        "        idx = random.randint(0, 4)\n",
        "        return self.dataset[idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = LandsatCustom('/content/drive/MyDrive/DLCVProj')"
      ],
      "metadata": {
        "id": "mQRMzMKmTQSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomGeoSampler(image, roi=torchgeo.datasets.BoundingBox(xmin, xmax, ymin, ymax, self.index.bounds[4], self.index.bounds[5]), size=256, length=5)"
      ],
      "metadata": {
        "id": "Jk5R6IvVVY_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEdcpKcJvCji"
      },
      "outputs": [],
      "source": [
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, instances_dict, len):\n",
        "        self.instances_dict = instances_dict\n",
        "        self.length = len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        chosen_key = random.choice(list(self.instances_dict.keys()))\n",
        "        chosen_instance = self.instances_dict[chosen_key]\n",
        "        sample = chosen_instance.get_samples()\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_6JHldCFI4O"
      },
      "outputs": [],
      "source": [
        "def plot_image(image):\n",
        "        rgb_indices = [3,2,1]\n",
        "        image = image[rgb_indices].permute(1, 2, 0)\n",
        "        image = ((image - 6000) / 6000).clip(0, 1).numpy()\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(image)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PQZMJJ0hCWt"
      },
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "\n",
        "# def is_file_corrupted(file_path):\n",
        "#     try:\n",
        "#         with open(file_path, 'rb') as file:\n",
        "#             pickle.load(file)\n",
        "#         return False  # File is not corrupted\n",
        "#     except (pickle.UnpicklingError, EOFError, IOError):\n",
        "#         return True  # File is corrupted\n",
        "\n",
        "# for img in os.listdir(\"/content/imgcollect\"):\n",
        "#     output_file_path = '/content/rsamples/' + img[:-4] + '.pkl'\n",
        "\n",
        "#     # Check if the output file already exists\n",
        "#     if Path(output_file_path).exists() and not is_file_corrupted(output_file_path):\n",
        "#         print(f\"Skipping {img} as the output file already exists and is not corrupted.\")\n",
        "#         continue\n",
        "\n",
        "#     print(\"starting new image\")\n",
        "#     imgfile = LandsatCustom(\"/content/imgcollect/\" + img)\n",
        "#     with open(output_file_path, 'wb') as file:\n",
        "#         pickle.dump(imgfile.dataset, file)\n",
        "\n",
        "#     gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78tkFyru3mFi"
      },
      "outputs": [],
      "source": [
        "# d = {}\n",
        "\n",
        "# for img in os.listdir(\"/content/imgcollect\"):\n",
        "#   name = img[:-4]\n",
        "#   d[\"{0}\".format(name)] = LandsatCustom(\"/content/imgcollect/\" + img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIbsgwP5S68g"
      },
      "outputs": [],
      "source": [
        "# train_dataset = CombinedDataset(d,2500)\n",
        "# val_dataset = CombinedDataset(d,500)\n",
        "# test_dataset = CombinedDataset(d,500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsBjjpWY9K3n"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(train_dataset, batch_size=64)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v43e7jCVYXNY"
      },
      "outputs": [],
      "source": [
        "weights = ResNet18_Weights.LANDSAT_OLI_SR_MOCO\n",
        "in_chans = weights.meta[\"in_chans\"]\n",
        "model = timm.create_model(\"resnet18\", in_chans=in_chans, num_classes=12)\n",
        "model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
        "model = model.to(device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UhjooVNrQBP"
      },
      "outputs": [],
      "source": [
        "epoch = 0\n",
        "val_accuracy = 0.0\n",
        "\n",
        "while val_accuracy < 96:\n",
        "      epoch += 1\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs).to(device)\n",
        "          labels -= 1\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      # Validate\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      model.eval()\n",
        "\n",
        "      for inputs, labels in val_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          labels -= 1\n",
        "          outputs = model(inputs).to(device)\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "          _,predicted = torch.max(outputs,1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      #Calculate Performance\n",
        "      val_loss /= len(val_loader)\n",
        "      val_accuracy = 100 * (correct / total)\n",
        "\n",
        "      print(f'Epoch {epoch}, Train Loss: {running_loss / len(train_loader)}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "best_weights = model.state_dict()\n",
        "torch.save(best_weights, 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "lB_4ilA4gJqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "val_accuracy = 0.0\n",
        "\n",
        "while val_accuracy < 98:\n",
        "      epoch += 1\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs).to(device)\n",
        "          labels -= 1\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      # Validate\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      model.eval()\n",
        "\n",
        "      for inputs, labels in val_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          labels -= 1\n",
        "          outputs = model(inputs).to(device)\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item()\n",
        "          _,predicted = torch.max(outputs,1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      #Calculate Performance\n",
        "      val_loss /= len(val_loader)\n",
        "      val_accuracy = 100 * (correct / total)\n",
        "\n",
        "      print(f'Epoch {epoch}, Train Loss: {running_loss / len(train_loader)}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "best_weights = model.state_dict()\n",
        "torch.save(best_weights, 'best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSZlSvJ-gNt9",
        "outputId": "492f3145-c48a-4ac6-cce5-6355cf45dfce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.02669930965550671, Val Accuracy: 95.76%\n",
            "Epoch 2, Train Loss: 0.018040026472822115, Val Accuracy: 97.04%\n",
            "Epoch 3, Train Loss: 0.012648368942414879, Val Accuracy: 98.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs).to(device)\n",
        "    _, predicted = torch.max(outputs,1)\n",
        "    total += labels.size(0)\n",
        "    labels -= 1\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * (correct / total)\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "0nNSEvO0jzHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GckA4B5Kf6SW"
      },
      "outputs": [],
      "source": [
        "# with open('/content/rsamples/LC08_015029_20190201.pkl', 'rb') as file:\n",
        "#     dataset = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1dSPrwW_heH"
      },
      "outputs": [],
      "source": [
        "# class LandsatCustom(RasterDataset):\n",
        "#     is_image = True\n",
        "#     separate_files = False\n",
        "#     all_bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B9\",\"B10\",\"B11\"]\n",
        "#     rgb_bands = [\"B4\", \"B3\", \"B2\"]\n",
        "#     filename_regex = r'(?P<satellite>[A-Za-z0-9]+)_(?P<number>\\d+)_(?P<date>(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2}))\\.tif'\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         raster_data = super(LandsatCustom, self).__getitem__(index)\n",
        "#         image = raster_data['image'][:7,:,:]\n",
        "#         label = datetime.utcfromtimestamp(self.index.bounds[4]).month\n",
        "#         return (image, label)\n",
        "\n",
        "#     def get_samples(self):\n",
        "#         xmin = self.index.bounds[0] + (self.index.bounds[1] - self.index.bounds[0])/5\n",
        "#         xmax = self.index.bounds[1] - (self.index.bounds[1] - self.index.bounds[0])/5\n",
        "#         ymin = self.index.bounds[2] + (self.index.bounds[3] - self.index.bounds[2])/5\n",
        "#         ymax = self.index.bounds[3] - (self.index.bounds[3] - self.index.bounds[2])/5\n",
        "\n",
        "#         newroi = torchgeo.datasets.BoundingBox(xmin, xmax, ymin, ymax, self.index.bounds[4], self.index.bounds[5])\n",
        "\n",
        "#         sampler = RandomGeoSampler(self, roi=newroi, size=256, length=1)\n",
        "#         dataset = []\n",
        "#         for index in sampler:\n",
        "#           sample = self[index]\n",
        "#           dataset.append(sample)\n",
        "#         return dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8gaQfZSa5RLV"
      },
      "outputs": [],
      "source": [
        "# !gsutil cp -r /content/rsamples/* gs://dlcv_finalproj_data1/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/random_samples'\n",
        "\n",
        "# List all files in the specified folder\n",
        "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.pkl')]\n",
        "combined_data = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data_chunk = pickle.load(file)\n",
        "        combined_data.extend(data_chunk)\n",
        "    file.close()\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/combined_data.pkl'\n",
        "\n",
        "# Save the combined data into a new .pkl file\n",
        "with open(output_file_path, 'wb') as output_file:\n",
        "    pickle.dump(combined_data, output_file)"
      ],
      "metadata": {
        "id": "_fIy1XfmS8iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/combined_data.pkl gs://dlcv_finalproj_data1/"
      ],
      "metadata": {
        "id": "o18-jxdvRUm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}